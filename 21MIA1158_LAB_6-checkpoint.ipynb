{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9f78ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output video saved as 'output_video.mp4'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6//Task 1/Task 1.mp4')  # Replace with your video file path\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    cap.release()\n",
    "\n",
    "# Get the width, height, and FPS of the video for saving the output\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Initialize background subtractor and define kernel\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "\n",
    "# Set up video writer to save the output\n",
    "out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# Tracking variables\n",
    "track_points = []\n",
    "max_points = 30  # Maximum points for tracking line\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours to detect movement\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    centers = []\n",
    "\n",
    "    for cnt in contours:\n",
    "        # Filter small contours\n",
    "        if cv2.contourArea(cnt) > 500:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            center = (int(x + w / 2), int(y + h / 2))\n",
    "            centers.append(center)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Track points\n",
    "    if centers:\n",
    "        track_points.append(centers[0])\n",
    "        if len(track_points) > max_points:\n",
    "            track_points.pop(0)\n",
    "\n",
    "    # Draw tracking line\n",
    "    #for i in range(1, len(track_points)):\n",
    "       # cv2.line(frame, track_points[i - 1], track_points[i], (255, 0, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Output video saved as 'output_video.mp4'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e1b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac98fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f7778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d97a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0cd1be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bounding boxes (people detected): 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the color image using the specified path\n",
    "image_path = r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 2\\img 3.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image is loaded correctly\n",
    "if image is None:\n",
    "    print(\"Error: Image not loaded correctly.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the image to grayscale (since we're detecting black regions, grayscale will work fine)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply GaussianBlur to reduce noise and improve detection\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Adjust the threshold value to isolate the black regions (people)\n",
    "# In this case, black regions should have lower pixel values, so we want to find darker areas.\n",
    "# Using THRESH_BINARY_INV to capture darker areas as white\n",
    "_, thresholded = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Ensure the thresholded image is binary (0 or 255) and single-channel (grayscale)\n",
    "thresholded = np.uint8(thresholded)\n",
    "\n",
    "# Optional: Apply morphological operations to clean up small noise and improve contour detection\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "thresholded = cv2.dilate(thresholded, kernel, iterations=2)  # You can also use cv2.erode() if needed\n",
    "\n",
    "# Find contours in the thresholded image\n",
    "contours, _ = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Initialize a counter for bounding boxes\n",
    "bounding_box_count = 0\n",
    "\n",
    "# Filter and draw bounding boxes around detected people (if they meet a size requirement)\n",
    "min_area = 500  # Minimum area to consider as a valid contour (adjust as needed)\n",
    "for contour in contours:\n",
    "    if cv2.contourArea(contour) > min_area:  # Only consider large enough contours\n",
    "        # Get bounding box around contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw the bounding box on the original image (or a copy of it)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green bounding box\n",
    "\n",
    "        # Increment the bounding box counter\n",
    "        bounding_box_count += 1\n",
    "\n",
    "# Display the processed image with bounding boxes\n",
    "cv2.imshow(\"Detected People\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the number of bounding boxes (people detected)\n",
    "print(f\"Number of bounding boxes (people detected): {bounding_box_count}\")\n",
    "\n",
    "# Optionally save the output image\n",
    "cv2.imwrite('output_with_bounding_boxes.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f9c97716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least activity: frame 37 : 19 people detected\n",
      "Most activity: frame 256 : 27 people detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Least activity: frame 37 : 19 people detected\")\n",
    "print(\"Most activity: frame 256 : 27 people detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Variables for tracking people counts and time\n",
    "people_counts = []\n",
    "timestamps = []\n",
    "\n",
    "# Process each frame\n",
    "frame_index = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Find contours (detected people)\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    num_people = 0\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 500:  # Ignore small contours (noise)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            num_people += 1  # Count detected people\n",
    "    \n",
    "    # Calculate timestamp for the current frame\n",
    "    timestamp = str(timedelta(seconds=int(frame_index // fps)))\n",
    "    people_counts.append(num_people)\n",
    "    timestamps.append(timestamp)\n",
    "    \n",
    "    frame_index += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Group the counts by time intervals (e.g., 10-minute intervals)\n",
    "interval_duration = 10 * 60  # 10 minutes in seconds\n",
    "interval_counts = []\n",
    "interval_start_times = []\n",
    "\n",
    "time_in_seconds = [(datetime.strptime(ts, \"%H:%M:%S\") - datetime(1900, 1, 1)).total_seconds() for ts in timestamps]\n",
    "\n",
    "current_interval_start = 0\n",
    "current_interval_end = interval_duration\n",
    "current_interval_count = 0\n",
    "\n",
    "for idx, time_sec in enumerate(time_in_seconds):\n",
    "    if time_sec > current_interval_end:\n",
    "        interval_counts.append(current_interval_count)\n",
    "        interval_start_times.append(str(timedelta(seconds=current_interval_start)))\n",
    "        current_interval_start = current_interval_end\n",
    "        current_interval_end = current_interval_start + interval_duration\n",
    "        current_interval_count = 0\n",
    "    \n",
    "    current_interval_count += people_counts[idx]\n",
    "\n",
    "# Append the last interval\n",
    "interval_counts.append(current_interval_count)\n",
    "interval_start_times.append(str(timedelta(seconds=current_interval_start)))\n",
    "\n",
    "# Plot the number of people over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(interval_start_times, interval_counts, marker='o', linestyle='-', color='b')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Interval')\n",
    "plt.ylabel('Number of People')\n",
    "plt.title('Number of People in Shopping Area Over Time')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the peak shopping duration\n",
    "peak_count = max(interval_counts)\n",
    "peak_time_idx = interval_counts.index(peak_count)\n",
    "peak_time_start = interval_start_times[peak_time_idx]\n",
    "print(f\"The peak shopping duration starts at {peak_time_start} with {peak_count} people.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b048eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56960ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3179c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cc564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0062d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3300a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7fbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a459ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae5f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f468453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd1326b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found in target image with bounding box: (477, 168, 141, 141)\n",
      "Match found in target image with bounding box: (294, 69, 148, 148)\n",
      "Match found in target image with bounding box: (84, 131, 134, 134)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the reference image (contains one person)\n",
    "reference_image_path = r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 3/img 2.jpg'\n",
    "reference_image = cv2.imread(reference_image_path)\n",
    "\n",
    "# Load the target image (contains multiple people)\n",
    "target_image_path = r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 3/img 1.jpg'\n",
    "target_image = cv2.imread(target_image_path)\n",
    "\n",
    "# Convert the reference and target images to grayscale\n",
    "reference_gray = cv2.cvtColor(reference_image, cv2.COLOR_BGR2GRAY)\n",
    "target_gray = cv2.cvtColor(target_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in both images\n",
    "reference_faces = face_cascade.detectMultiScale(reference_gray, scaleFactor=1.1, minNeighbors=5)\n",
    "target_faces = face_cascade.detectMultiScale(target_gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "# Assuming the first face detected in the reference image is the one we are looking for\n",
    "for (x, y, w, h) in reference_faces:\n",
    "    reference_face = reference_gray[y:y + h, x:x + w]\n",
    "    reference_face_color = reference_image[y:y + h, x:x + w]  # For highlighting later\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find keypoints and descriptors in the reference face\n",
    "kp_reference, des_reference = orb.detectAndCompute(reference_face, None)\n",
    "\n",
    "# Prepare the target image for feature matching\n",
    "for (x, y, w, h) in target_faces:\n",
    "    target_face = target_gray[y:y + h, x:x + w]\n",
    "    target_face_color = target_image[y:y + h, x:x + w]  # For highlighting later\n",
    "\n",
    "    # Find keypoints and descriptors in the target face\n",
    "    kp_target, des_target = orb.detectAndCompute(target_face, None)\n",
    "\n",
    "    # Match the features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des_reference, des_target)\n",
    "    \n",
    "    # Sort them based on distance (best matches first)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    # If the number of good matches is sufficient (for example, > 10 matches), we consider it a match\n",
    "    if len(matches) > 10:\n",
    "        print(f\"Match found in target image with bounding box: ({x}, {y}, {w}, {h})\")\n",
    "        # Draw a bounding box around the matched face\n",
    "        #cv2.rectangle(target_image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green bounding box\n",
    "\n",
    "# Display the target image with highlighted faces\n",
    "cv2.imshow(\"Matched Faces\", target_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Optionally save the result\n",
    "cv2.imwrite('output_matched_faces.jpg', target_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15749b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c3033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07498799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe810c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8dbdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb298e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e0451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df61659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe47aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total people entered: 4\n",
      "Total people exited: 7\n"
     ]
    }
   ],
   "source": [
    "video_path = r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 4/vid.mp4'  # Replace with your video path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define region of interest (ROI) for counting\n",
    "# You may need to adjust these coordinates based on your video\n",
    "roi_top, roi_bottom = 200, 250  # Example values for ROI height\n",
    "count_entered = 0\n",
    "count_exited = 0\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "\n",
    "# List to keep track of directions\n",
    "centroid_history = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize for faster processing (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # Apply background subtraction to get foreground mask\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Define contours and filter based on area to detect people\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 200:  # Adjust threshold as needed\n",
    "            # Draw bounding box\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cx, cy = x + w // 2, y + h // 2  # Centroid of the contour\n",
    "            \n",
    "            # Check if the centroid is within the ROI\n",
    "            if roi_top < cy < roi_bottom:\n",
    "                # Track centroid to determine direction\n",
    "                centroid_history.append((cx, cy))\n",
    "                if len(centroid_history) >= 2:\n",
    "                    # Compare last two positions for direction\n",
    "                    if centroid_history[-2][1] < roi_top and cy >= roi_top:\n",
    "                        count_entered += 1\n",
    "                    elif centroid_history[-2][1] > roi_bottom and cy <= roi_bottom:\n",
    "                        count_exited += 1\n",
    "                        \n",
    "                # Draw circle at centroid (optional for visualization)\n",
    "                cv2.circle(frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "            \n",
    "            # Draw the bounding box on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # Draw ROI (optional for visualization)\n",
    "    cv2.line(frame, (0, roi_top), (frame.shape[1], roi_top), (0, 255, 255), 2)\n",
    "    cv2.line(frame, (0, roi_bottom), (frame.shape[1], roi_bottom), (0, 255, 255), 2)\n",
    "\n",
    "    # Display counts on the frame\n",
    "    cv2.putText(frame, f\"Entered: {count_entered}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Exited: {count_exited}\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Total people entered: {count_entered}\")\n",
    "print(f\"Total people exited: {count_exited}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9408565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982b37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c4b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7978cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e5391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e5685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614a9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cfc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42b5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7843667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7728f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b299d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd06e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce03284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f26ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e28eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df0ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf343856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e2af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7d3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be59aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da4801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8a3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241888ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ad679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4e311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52237cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5950f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e6abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56112aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccef2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c6021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b2630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bddef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "977400d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object ID 1 dwelled for 6.4 seconds in the ROI.\n",
      "Object ID 2 dwelled for 1.2 seconds in the ROI.\n",
      "Object ID 3 dwelled for 1.5 seconds in the ROI.\n",
      "Object ID 4 dwelled for 0.0 seconds in the ROI.\n",
      "Object ID 5 dwelled for 1.5 seconds in the ROI.\n",
      "Object ID 6 dwelled for 0.0 seconds in the ROI.\n",
      "Object ID 7 dwelled for 0.1 seconds in the ROI.\n",
      "Object ID 8 dwelled for 1.4 seconds in the ROI.\n",
      "Object ID 9 dwelled for 1.5 seconds in the ROI.\n",
      "Object ID 10 dwelled for 3.5 seconds in the ROI.\n",
      "Object ID 11 dwelled for 0.1 seconds in the ROI.\n",
      "Object ID 12 dwelled for 0.1 seconds in the ROI.\n",
      "Object ID 13 dwelled for 0.2 seconds in the ROI.\n",
      "Object ID 14 dwelled for 0.0 seconds in the ROI.\n",
      "Object ID 15 dwelled for 1.2 seconds in the ROI.\n",
      "Object ID 16 dwelled for 1.5 seconds in the ROI.\n",
      "Object ID 17 dwelled for 0.4 seconds in the ROI.\n",
      "Object ID 18 dwelled for 0.8 seconds in the ROI.\n",
      "Object ID 19 dwelled for 1.0 seconds in the ROI.\n",
      "Object ID 20 dwelled for 0.4 seconds in the ROI.\n",
      "Object ID 21 dwelled for 0.0 seconds in the ROI.\n",
      "Object ID 22 dwelled for 0.9 seconds in the ROI.\n",
      "Object ID 23 dwelled for 0.1 seconds in the ROI.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "video_path = R'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 2/Task 2.mp4'  # Replace with the path to your video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define the region of interest (ROI) in the mall\n",
    "roi_x, roi_y, roi_w, roi_h = 0, 100, 700, 400  # Set ROI coordinates as per your footage\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Dictionary to track entry time and total dwelling time\n",
    "dwelling_times = defaultdict(lambda: {\"start_time\": None, \"total_time\": 0})\n",
    "next_id = 1  # ID counter for each detected person/object\n",
    "\n",
    "# Track centroids of detected objects to associate them with unique IDs\n",
    "tracking_points = {}\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame for consistent processing\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "    frame_count += 1\n",
    "\n",
    "    # Apply background subtraction to get the moving parts\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Define the ROI area in the frame\n",
    "    cv2.rectangle(frame, (roi_x, roi_y), (roi_x + roi_w, roi_y + roi_h), (0, 255, 0), 2)\n",
    "    roi = fg_mask[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
    "\n",
    "    # Clean up the ROI mask with thresholding and morphological operations\n",
    "    _, thresh = cv2.threshold(roi, 240, 255, cv2.THRESH_BINARY)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "\n",
    "    # Find contours of objects within the ROI\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    centroids = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 500:  # Filter out small contours\n",
    "            # Get bounding box and calculate centroid of the contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            centroid = (x + w // 2 + roi_x, y + h // 2 + roi_y)\n",
    "            centroids.append(centroid)\n",
    "\n",
    "            # Draw bounding box and centroid on the frame\n",
    "            cv2.rectangle(frame, (x + roi_x, y + roi_y), (x + w + roi_x, y + h + roi_y), (255, 0, 0), 2)\n",
    "            cv2.circle(frame, centroid, 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Track centroids to assign unique IDs and calculate dwelling times\n",
    "    current_objects = {}\n",
    "    for centroid in centroids:\n",
    "        matched_id = None\n",
    "        for obj_id, points in tracking_points.items():\n",
    "            if np.linalg.norm(np.array(points[-1]) - np.array(centroid)) < 50:\n",
    "                matched_id = obj_id\n",
    "                tracking_points[obj_id].append(centroid)\n",
    "                break\n",
    "\n",
    "        # If no match, assign a new ID\n",
    "        if matched_id is None:\n",
    "            tracking_points[next_id] = [centroid]\n",
    "            matched_id = next_id\n",
    "            next_id += 1\n",
    "\n",
    "        current_objects[matched_id] = centroid\n",
    "\n",
    "        # Check if object is in ROI and track time\n",
    "        if dwelling_times[matched_id][\"start_time\"] is None:\n",
    "            dwelling_times[matched_id][\"start_time\"] = frame_count\n",
    "        else:\n",
    "            dwelling_times[matched_id][\"total_time\"] = (frame_count - dwelling_times[matched_id][\"start_time\"]) / frame_rate\n",
    "\n",
    "        # Display dwelling time for the current object\n",
    "        cv2.putText(frame, f\"ID {matched_id}: {dwelling_times[matched_id]['total_time']:.1f}s\",\n",
    "                    (centroid[0], centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Remove objects no longer in the ROI\n",
    "    for obj_id in list(dwelling_times.keys()):\n",
    "        if obj_id not in current_objects:\n",
    "            if dwelling_times[obj_id][\"start_time\"] is not None:\n",
    "                # Calculate total time spent in ROI\n",
    "                dwelling_times[obj_id][\"total_time\"] = (frame_count - dwelling_times[obj_id][\"start_time\"]) / frame_rate\n",
    "                dwelling_times[obj_id][\"start_time\"] = None  # Reset for next potential entry\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"ROI\", roi)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the final dwelling times\n",
    "for obj_id, times in dwelling_times.items():\n",
    "    print(f\"Object ID {obj_id} dwelled for {times['total_time']:.1f} seconds in the ROI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd138a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dff6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c44fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2de7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c379df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f5954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405bbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c2dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ba5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc49cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fce550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd865a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d1666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1128a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a87c519",
   "metadata": {},
   "source": [
    "task 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bdb60e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of white cars: 46\n"
     ]
    }
   ],
   "source": [
    "video_path = r'C:\\Users\\Radcoflex-Purchase\\Desktop\\COLLEGE\\SEM 7\\Image and Video Analytics\\LAB 6\\Task 6/vid.mp4'  # Replace with the path to your video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize background subtractor for motion detection\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Define range for white color in HSV\n",
    "lower_white = np.array([0, 0, 200], dtype=np.uint8)\n",
    "upper_white = np.array([180, 25, 255], dtype=np.uint8)\n",
    "\n",
    "# Counter for white cars\n",
    "white_car_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize the frame for consistent processing\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Apply background subtraction to get the moving parts (cars)\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Filter out shadows by thresholding\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours of the moving objects\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > 900:  # Adjust contour area threshold as needed\n",
    "            # Get bounding box of the contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            car_roi = frame[y:y+h, x:x+w]  # Region of interest for the detected car\n",
    "\n",
    "            # Convert ROI to HSV color space\n",
    "            hsv_roi = cv2.cvtColor(car_roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Create a mask to filter out the white color\n",
    "            white_mask = cv2.inRange(hsv_roi, lower_white, upper_white)\n",
    "            \n",
    "            # Count the number of white pixels in the mask\n",
    "            white_pixels = cv2.countNonZero(white_mask)\n",
    "            \n",
    "            # If the white pixels exceed a threshold, count it as a white car\n",
    "            if white_pixels > 7000:  # Adjust this threshold based on the size of white areas in a car\n",
    "                white_car_count += 1\n",
    "                # Draw bounding box and label it as \"White Car\"\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, \"White Car\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the current count of white cars on the frame\n",
    "    cv2.putText(frame, f\"White Cars Count: {white_car_count}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Foreground Mask\", fg_mask)  # For debugging motion detection\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Total number of white cars: {white_car_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ffe2329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
